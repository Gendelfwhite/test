ГЛАВА 9. Нейросетевые архитектуры на основе Transformer
Трансформеры – это относительно новый тип нейросетей, основанный на использование механизма внимания. На сегодня это самая продвинутая техника в области обработки естественной речи (NLP). Задача машинного перевода (как и многие другие задачи) в deep learning
сводится к работе с последовательностями: мы тренируем модель, которая может получить на
вход предложение как последовательность слов на одном языке и выдать последовательность
слов на другом языке. Для решения этой задачи в текущих подходах модель обычно состоит
из энкодера и декодера (рис. 86). Энкодер преобразует слова входного предложения в один
или больше векторов в некоем векторном пространстве, декодер – генерирует из этих векторов
последовательность слов на другом языке. До 2017 года в задачах глубокого обучения, связанного с пониманием текста, обычно использовали рекуррентные нейронные сети. Однако использование такого подхода имеет ряд проблем.
Во-первых, RNN плохо обрабатывают большие последовательности текста. Например,
к моменту продвижения к концу абзаца они «забывают» содержание начала.
Во-вторых, RNN сложно и долго обучаются, также они подвержены так называемой
проблеме исчезающего или взрывающегося градиента.
В-третьих, они обрабатывают данные последовательно, поэтому рекуррентную нейросеть
трудно распараллелить. Ускорить обучение RNN, используя больше графических процессоров
нельзя, поэтому ее практически невозможно обучить на большом количестве данных.
Рис. 86. Архитектура Encoder -decoder
В отличие от рекуррентных нейронных сетей, трансформеры не обрабатывают последовательности по порядку, используя механизм внимания (attention). Благодаря использова-
87
нию внимания трансформеры можно распараллелить и обучить значительно быстрее. Механизм внимания в нейронных сетях является техникой, которая позволяет сети фокусироваться
на определенных частях входных данных в процессе обработки информации. Этот механизм
приобрел большую популярность в области обработки естественного языка (Natural Language
Processing, NLP), особенно в машинном переводе, где сети должны обрабатывать входные последовательности переменной длины. Основная идея механизма внимания заключается в том,
чтобы давать модели возможность сосредотачиваться на различных частях входных данных в
зависимости от их важности для текущей задачи. Это достигается путем присвоения весов
различным элементам входных данных на основе их влияния на результат.
Рассмотрим механизм внимания более подробно. Предположим, у нас есть три входные
последовательности XQ, XK, XV, каждая из которых имеет длину T элементов. Нам необходимо получить вектор контекста С, учитывая веса внимания для каждого элемента последовательности. Алгоритм состоит из следующих шагов:
1. Вычисление тензоров трех преобразованных представлений входных данных для
Query (запрос), Key (ключ) и Value (значение) – Q, K V. Вычисления проводятся по формулам:
Q=XQ*WQ, K=XK*Wk, V=XV*WV, где WQ, Wk, WV – это матрицы весов для Query, Key и Value
соответственно.
2. Расчет весов внимания. Сначала вычисляем «сырые» веса внимания eij между Qi и
Kk по формуле eij=Score(Qi, Kj) = Qi*Kj
T. Далее применяем функцию softmax для получения
нормализованных весов внимания: aij=
𝐞𝐞𝐞𝐞𝐞𝐞�𝒆𝒆𝒊𝒊𝒊𝒊�
∑ 𝐞𝐞𝐞𝐞𝐞𝐞(𝒆𝒆𝒊𝒊𝒊𝒊) 𝑻𝑻
𝒕𝒕=𝟏𝟏
.
3. Вычисления контекстного вектора – взвешенную сумму тензора value V с использованием весов внимания: С𝐢𝐢 = ∑ 𝐚𝐚𝐢𝐢𝐢𝐢𝐯𝐯𝐣𝐣
𝐓𝐓
𝐣𝐣=𝟏𝟏
Механизм самовнимания (Self-Attention) представляет собой частный случай механизма внимания, где значения Query, Key и Value получаются из одной и той же последовательности. Self-Attention особенно полезен для обработки последовательностей, где важные
элементы могут быть в разных частях последовательности.
Трансформеры используют особый вид слоя, известный как многоголовое внимание
(Multi-head attention) (рис. 87). В это случает несколько механизмов внимания тренируются
параллельно, что означает несколько линейных преобразований и параллельных операций
скалярного произведения и взвешенной суммы.
88
Рис. 87. Схема многоголового внимания
Результаты всех параллельных механизмов внимания конкатенируются, затем проходят через еще одно обучаемое линейное преобразование и поступают на выход. В целом каждый такой модуль получает вектор запроса и набор векторов ключей и значений, выводя при
этом один вектор того же размера, что и каждый из входов. Многоголовое внимание позволяет
модели совместно воспринимать информацию из разных подпространств представлений на
разных позициях. При одной голове внимания усреднение препятствует этому.
Рис. 88. Архитектура трансформера
89
Изучим подробно строение и функционирование трансформера на примере задачи перевода текста с одно языка на другой. Трансформер состоит из следующих основных блоков
(рис. 88):
1. Механизм внимания (Attention Mechanism). Использование этого механизма позволяет модели фокусироваться на различных частях входных данных при выполнении задачи.
Трансформер использует механизм многоголового внимания, который позволяет модели параллельно обрабатывать информацию из разных представлений входных данных, повышая её
эффективность и глубину понимания.
2. Позиционное кодирование (Positional Encoding). Поскольку трансформер не использует рекуррентность или свёртки для обработки последовательностей, ему необходим
способ учёта порядка элементов в последовательности. Для этого вводные данные дополняются позиционным кодированием, которое содержит информацию о позиции элемента в последовательности.
3. Блоки энекодера и декодера (Encoder and Decoder Blocks): Архитектура трансформера состоит из стеков блоков энкодеров и декодеров. Каждый блок энкодера содержит
две основные части: слой многоголового внимания и позиционно-зависимую полносвязную
сеть (pointwise feed-forward neural network). Между этими частями и после них применяется
нормализация по слоям (layer normalization). Блоки декодера аналогично содержат три основные части: механизм многоголового внимания, многоголовое внимание, направленное на выходы кодировщика, и позиционно-зависимую полносвязную сеть. На вход декодера подается
выход энкодера. Главное отличие архитектуры декодера от энкодера заключается в том, что
дополнительно имеется внимание к вектору, который получен из последнего блока кодирующего компонента. Компонент декодера тоже многослойный и каждому блоку компонента на
вход подается вектор именно с последнего блока кодирующего компонента.
4. Cквозные (residual) связи и нормализация по слою (Add & Norm). Каждый подслой (например, многоголовое внимание или полносвязный слой) в блоках кодировщика и декодировщика снабжается сквозными связями (Residual Connection) и последующей нормализацией по слою (Layer Normalization). Это улучшает обучение, предотвращая проблему исчезающего градиента и улучшая поток градиентов через сеть.
5. Выходной слой. В декодере последний слой перед выходом обычно представляет
собой полносвязный слой (FC), который преобразует векторы в вероятности символов или
других элементов выходных данных. Наконец, в конце сети присутствует стандартный
softmax слой, который вычисляет вероятности для каждого слова.
90
Рассмотрим пример реализации трансформера на основе библиотеки PyTorch. Полный
пример вы можете найти в репозитории по адресу:
https://github.com/fgafarov1977/pytorch_nn_tutorial/blob/main/transformer_translator.ipynb.
Мы построим модель, а также напишем код ее обучения для решения задачи перевода
текста с английского языка на русский язык. Для обучения трасформера решению задачи перевода текста с одного языка на другой необходим советующий набор данных, который должен содержать предложения на английском и русском языках. Для обучения и тестирования
модели используем известный набор данных opus_books, который расположен по адресу:
https://huggingface.co/datasets/opus_books.
В самом начале программы добавим классы для реализации класса для нормализации
по слоям NormalizationLayer и классы FeedForwardBlock и ProjectionLayer, которые будут
служить строительными блоками в разных частях трансформера. Нормализация по слоям
(NormalizationLayer) приводит активации каждого слоя к нулевому среднему и единичному
стандартному отклонению, что способствует более стабильному распределению данных и
предотвращает проблемы, связанные с внутренними сдвигами и масштабами. Модуль
FeedForwardBlock состоит из двух линейных слоев, разделенных функцией активации ReLU
и слоем дропаута. Каждый слой трансформера включает этот модуль после применения механизма внимания. Задача модуля FeedForwardBlock – обрабатывать контекстные векторы, полученные из механизма внимания, и создавать новые, более сложные представления для каждого токена. Этот блок позволяет трансформеру улавливать нелинейные зависимости между
токенами и вносить дополнительную гибкость в модель. ProjectionLayer – это слой, который
будет применяться к выходным представлениям декодера. Задача этого слоя – преобразовать
выходные представления в пространство, соответствующее размеру словаря, он включает в
себя линейное преобразование для последующего применения функции softmax для получения вероятностного распределения по выходному словарю или классам.
class NormalizationLayer(nn.Module):
 def __init__(self, features: int, eps:float=10**-6) -> None:
 super().__init__()
 self.eps = eps
 self.alpha = nn.Parameter(torch.ones(features))
 self.bias = nn.Parameter(torch.zeros(features))
 def forward(self, x):
 mean = x.mean(dim = -1, keepdim = True)
 std = x.std(dim = -1, keepdim = True)
 return self.alpha * (x - mean) / (std + self.eps) + self.bias
class FeedForwardBlock(nn.Module):
 def __init__(self, d_model: int, d_hidden: int, dropout: float) -> None:
 super().__init__()
 self.linear1 = nn.Linear(d_model, d_hidden)
 self.dropout = nn.Dropout(dropout)
 self.linear2 = nn.Linear(d_hidden, d_model)
 def forward(self, x):
91
return self.linear2(self.dropout(torch.relu(self.linear1(x))))
class ProjectionLayer(nn.Module):
 def __init__(self, d_model, vocab_size) -> None:
 super().__init__()
 self.proj = nn.Linear(d_model, vocab_size)
 def forward(self, x) -> None:
 return self.proj(x)
Далее создадим модуль, реализующий класс InputEmbBlock, который принимают
входные данные и преобразуют их в векторное представления, называемые входными эмбедингами. С помощью этого модуля сначала мы преобразуем предложение на английском
языке в список входных идентификаторов слов, то есть в список чисел, соответствующих положению каждого слова в словаре, а затем переводим каждое из этих чисел в вектора размера
512. В конструктор этого класса передаем размер модели (d_model), а также размер словаря
(vocab_size). В PyTorch уже имеется слой, который позволяет вычислять эмбединги, этот слой
называется Embedding, поэтому мы его включим в этот класс. В методе forward модели мы
пропускаем входной тензор через слой Embedding и умножаем на корень квадратный из размера модели.