ГЛАВА 3. Однослойные и многослойные нейронные сети в PyTorch Искусственные нейронные сети состоят из узлов, которые называются нейронами. На рисунке 23 изображена схема формального нейрона. У нейрона может быть много входов, но только один выход. Через входы на нейрон поступают входные данные (признаки), а на выходе формируется результат работы. Нейрон вычисляет взвешенную сумму входных сигналов, а затем преобразует полученную сумму с помощью заданной нелинейной функции, которая называется функцией активации нейрона. Множество, состоящее из порогового уровня и всех весов, называют параметрами нейрона. Количество весовых коэффициентов нейрона равно количеству входов нейрона. Данные, подаваемые на вход нейрона, обычно называют входным паттерном, он обычно бывает в виде массива (или тензора) чисел. Рис. 23. Формальный нейрон Функция активации определяет выходное значение нейрона в зависимости от результата взвешенной суммы входов и порогового значения. На рис. 24 в качестве примеров приведены графики классических функции активации. Пороговая функция активации является одной из наиболее простых функций активации. Результат этой функции равен единице, если значение аргумента выше определенного значения, которое называется пороговым, иначе результат функции равен нулю. В этом случае нейрон может выдавать на выходе только два значения: 0 и 1. Линейная функция активация представляет собой прямую. Такой выбор активационной функции позволяет получать спектр значений, а не только бинарный ответ. 29 Рис. 24. Пороговая и линейная функции активации В настоящее время в библиотеке PyTorch реализованы десятки различных функций активации. Однако на практике чаще всего используются только несколько из них. На рисунке 25 приведены графики наиболее часто используемых функций активации, перечисленных ниже. 1. Rectified Linear Unit (ReLu). Эта функция возвращает значение аргумента, если его значение положительно, и 0 – в противном случае. 2. Сигмоида (sigmoid). Сигмоида выглядит гладкой и подобна ступенчатой функции. Ее преимуществами является то, что сигмоида – это нелинейная функция по своей природе, а комбинация таких функций производит тоже нелинейную функцию. Еще одно преимущество этой функции, это то, что она не бинарна, это делает активацию аналоговой, в отличие от ступенчатой функции. 3. Гиперболический тангенс (tanh). Гиперболический тангенс очень похож на сигмоиду. Эта функция тоже нелинейная, она хорошо подходит для комбинации слоев, а диапазон значений функции -(-1, 1). 4. Softplus. Эта функция представляет собой плавное приближение к функции активации ReLU поэтому иногда используется в нейронных сетях вместо ReLU. 30 Рис. 25. Графики функций активации (ReLu, sigmoid, tanh, softplus) На рис. 26 приведен программный код, демонстрирующий вычисление этих функции методами библиотеки PyTorch (программный код расположен по адресу: https://github.com/fgafarov1977/pytorch_nn_tutorial/blob/main/atcivation_functions.ipynb). Рис. 26. Вычисление значений функций активации: ReLu, sigmoid, tanh, softplus Один нейрон может выполнять простейшие вычисления, но основные функции нейросети обеспечиваются не отдельными нейронами, а сетями нейронов. Наиболее простой нейронной сетью является однослойный перцептрон, который представляет собой простейшую нейронную сеть, состоящую из группы нейронов, образующих слой (рис. 27). Входные данные нейронной сети кодируются вектором значений, каждый элемент подается на соответ- 31 ствующий вход каждого нейрона в слое. В свою очередь, нейроны вычисляют выход независимо друг от друга. В однослойных нейронных сетях размерность выхода (то есть количество выходных элементов) равна количеству нейронов, а количество связей у всех нейронов должно быть одинаково и совпадать с размерностью входного сигнала. Здесь X1, X2, X3 – это элементы входного паттерна, а Y1, Y2, Y3 – элементы выходного паттерна, а w_(i,j)- это j-ый весовой коэффициент i-го нейрона (см. рис. 28). Рис. 27. Однослойная нейронная сеть Одним из наиболее простых методов обучения нейронных сетей является метод градиентного спуска в пространстве весовых коэффициентов. Градиентный спуск – это итеративный метод нахождения локального экстремума (минимума или максимума) функции с помощью движения вдоль градиента. На рис. 29 приведены формулы и схематическое изображение метода градиентного спуска, показывающие изменение значений весовых коэффициентов и пороговых значений на каждом шаге итерации обучения. Здесь E – это функционал ошибок, который также еще называется функцией потерь, а alpha – это шаг обучения. При использовании этого метода сначала вычисляются градиенты от функции потерь по параметрам модели. Потом параметры модели уменьшаются на величину соответствующих вычисленных градиентов, умноженных на величину шага обучения.