В этой главе мы изучим рекуррентные нейронные сети. Вначале мы рассмотрим основные особенности и области применения рекуррентных нейронных сетей, изучим алгоритм работы обычной рекуррентной нейронной сети (RNN). Кратко коснемся основных моментов касательно обучения рекуррентных нейронных сетей, рассмотрим LSTM нейронные сети. И в конце рассмотрим RNN и LSTM в слоях библиотеки PyTorch и режимы работы рекуррентной нейронной сети (RNN). Ранее мы познакомились с основными принципами построения архитектур полносвязных однослойных и многослойных нейронных сетей. Такие нейронные сети состоят из одного слоя или группы слоёв и принимают на вход тензор фиксированного размера. Каждый слой применяет к этому объекту какие-либо преобразования, и на выходе мы получаем результат. Но в некоторых областях машинного обучения нам необходимо иметь большую гибкость в типах данных, которые мы могли бы обрабатывать. Классические нейронные сети без рекуррентных связей не могут запоминать информацию, и это является их главным недостатком. В полносвязных нейронных сетях с прямыми связами подразумевается, что все входы и выходы независимы. Например, если необходимо классифицировать события, происходящие в каждом кадре фильма. Или, например, если вы хотите предсказать следующее слово в предложении, то необходимо учитывать предшествующие ему слова. Непонятно, как классическая нейронная сеть может использовать свои предыдущие выводы для дальнейших решений. Есть много задач, которые невозможно решить с помощью такого подхода. Именно это позволяют делать рекуррентные нейронные сети. Рекуррентные нейронные сети – это сети с обратными или перекрестными связями между различными слоями нейронов. При этом под обратной связью подразумевается связь от логически более удалённого элемента к менее удалённому. Идея RNN заключается в последовательном использовании информации. Рекуррентные нейронные сети применяются для решения задач языкового моделирования и генерации текстов, в машинном переводе, для распознавания речи, для генерации описания изображений, прогнозирования следующего кадра на основе предыдущих, в разнообразных задачах, связанных с анализом текстов, для обработки последовательности звуков, при прогнозировании временных рядов, и во многих других задачах. Рекуррентные нейронные сети выполняют одинаковые вычисления для каждого элемента последовательности, причем выход зависит от предыдущих вычислений. Т. е. рекуррентные нейронные сети, это сети, у которых есть «память», учитывающая предшествующую информацию. Эта память описывается в виде скрытого состояния h. Такие нейронные сети 48 могут использовать информацию о произвольно длинных последовательностях, хотя на практике они ограничены лишь несколькими шагами (см. рис. 45). Вычисления в большинстве типовых рекуррентных сетей можно разложить на три преобразования: 1) преобразование входа x в скрытое состояние h; 2) преобразование от предыдущего скрытого состояния в следующее скрытое состояние; 3) преобразование скрытого состояния h в выход y. На рис. 46 приведены формулы, по которым вычисляется значение скрытого состояния нейронной сети и выходное значение. Рис. 45. Алгоритм вычислений обычной рекуррентной нейронной сети Из-за циклов рекуррентные нейронные сети становятся трудными в понимании и в обучении. Рекуррентную сеть можно развернуть в последовательность одинаковых обыкновенных нейронных сетей, передающих информацию к последующим. На диаграмме (см. рис. 46) показано, как рекуррентная нейронная сеть разворачивается во времени. Разверткой мы просто выписываем сеть до полной последовательности. Например, если последовательность состоит из 3 элементов, то развертка будет состоять из 3 слоев, по слою на каждый элемент данных. Здесь x(t) – входное значение на временном шаге t; x(t-1) – входное значение на предыдущем временном шаге; x(t+1) – входное на последующем временном шаге. h(t) – это скрытое состояние на шаге времени t. h(t) зависит, как функция, от предыдущих состояний h(t-1) и от текущего входного значения x(t). y(t) – выходное значение на шаге t (см. рис. 46). 49 Рис. 46. Схема рекуррентной нейронной сети развернутой во времени На вход нейронной сети поступают элементы последовательности. На вход первой копии поступает первый элемент последовательности входных данных, следующий элемент поступает на вход второй копии и так далее. Рекуррентная нейронная сеть возвращает два значения. Первое значение – это выходное значение, которое поступает на выход из нейронной сети. Также рекуррентная нейронная сеть выдает второе значение – h, которое поступает на вход следующей копии нейронной сети. Следующая копия нейронной сети на вход получает второй элемент последовательности x, а также скрытое состояние с предыдущего этапа. Вторая копия нейронной сети анализирует одновременно текущий элемент последовательности и данные со скрытого состояния предыдущей копии нейронной сети и, в зависимости от результатов анализа, также выдает два значения: выходное значение и скрытое состояние, которое передается следующей копии нейронной сети. И так продолжается, пока мы не дойдем до последнего элемента данных в последовательности. Для него рекуррентная сеть выдает уже одно значение, которое подается на выход без скрытого состояния. Рекуррентные нейронные сети могут состоять из множества последовательных рекуррентных слоев. На рис. 47 схематично изброжена многослойная рекуррентная нейронная сеть. Сначала входные данные подаются на первый слой. Выходные значения первого слоя передаются на следующий слой. Таким образом осуществляется последовательная обработка информации всеми слоями нейронной сети, и выход последнего слоя будет выходом всей нейронной сети. 50 Рис. 47. Схема многослойной рекуррентной нейронной сети развернутой во времени Обучение RNN аналогично обучению обычной полносвязной нейронной сети. Здесь также используется алгоритм обратного распространения ошибки, однако с небольшим изменением в связи с особенностями рекуррентной нейронной сети. Поскольку одни и те же параметры используются на всех временных этапах в сети, градиент на каждом выходе зависит не только от расчетов текущего шага, но и от предыдущих временных шагов. Например, чтобы вычислить градиент при t = 3, нам нужно было бы провести обратное распространение ошибки на 3 предыдущих шага и суммировать градиенты (см. рис. 48). Такой алгоритм называется «алгоритмом обратного распространения ошибки сквозь время». Необходимо отметить, что рекуррентные нейронные сети испытывают проблемы при обучении из-за затухания или взрывания градиентов. Для того, чтобы обойти эти проблемы, были разработаны специальные архитектуры RNN (например LSTM сети). Рис. 48. Схема «алгоритма обратного распространения ошибки сквозь время» 51 Долгая краткосрочная память (Long short-term memory; LSTM) – особая разновидность архитектуры рекуррентных нейронных сетей, способная к обучению долговременным зависимостям. Они успешно решают целый ряд разнообразных задач и в настоящее время широко используются. Эти нейронные сети разработаны специально, чтобы избежать проблемы запоминания долговременных зависимостей. Структуру LSTM также можно изобразить в виде цепочки, но модули выглядят иначе (см. рис. 49). Вместо одного слоя нейронной сети они содержат целых четыре, которые взаимодействуют особенным образом. Рис. 49. Схема LSTM нейронной сети На рис. 50 подробно изображена внутренняя структура LSTM ячейки, а также приведены формулы, описывающие обработку информации внутри нее. Ключевым понятием здесь является состояние ячейки – это горизонтальная линия, проходящая через верхнюю часть диаграммы. В LSTM ячейках уменьшается или увеличивается количество информации в состоянии ячейки, в зависимости от потребностей. Для этого используются настраиваемые структуры, пропускающие или не пропускающие информацию, которые называются гейтами или воротами. Гейты состоят из сигмовидного слоя и операции поточечного умножения. На первом шаге необходимо определить, какую информацию можно удалить из состояния ячейки. Это выполняется сигмоидальным слоем, называемый «слоем забывания» (forget gate layer). Этот гейт на основе предыдущего скрытого состояния ht-1 и текущего входа xt и возвращает число от 0 до 1, 1 означает «полностью сохранить», а 0 – «полностью забыть». На следующем шаге принимается решение о том, какая новая информация будет храниться в состоянии ячейки. На этом шаге сначала сигмоидальный слой под названием «слой входного фильтра» (input layer gate) определяет, какие значения следует обновить. Затем слой, в котором используется тангенциальная функция активации, строит вектор новых значенийкандидатов, которые можно добавить в состояние ячейки. Затем старое состояние ячейки Ct-1 заменяется на новое состояние Ct. Для этого старое состояние умножается на ft, и прибавляется it, умноженное на gt. Выходное значение будет вычисляться основе состоянии ячейки, с применением некоторых фильтров. Сначала применятся сигмоидальный слой, затем значения состояния ячейки 52 проходят через слой с активационной функцией гиперболический тангенс, чтобы получить на выходе результат в диапазоне от -1 до 1, и далее перемножаются с выходными значениями сигмоидального слоя, что позволяет выводить только требуемую информацию. Рис. 50. Схематическое изображение LSTM ячейки На рис. 51 показаны примеры использования RNN и LSTM слоев библиотеки PyTorch Классический рекуррентный RNN слой добавляется как функция torch.nn.RNN(), LSTM слой добавляется через функцию torch.nn.LSTM(). Основные параметры, которые необходимо указывать при добавлении этих слоев в нейронную сеть, это input_size – размерность входа, hidden_size – размерность скрытого состоянии и num_layers – количество повторяющихся слоев. Например, если укажем количество слоев num_layers = 2, это будет означать двухслойную нейронную сеть, в котором второй слой будет принимать выходные значения первого слоя в качестве входных значений. Рис. 51. RNN и LSTM слои в PyTorch Существует четыре основных режима работы и использования рекуррентных нейронных сетей (см. рис. 52). 53 Первый режим называется «многие ко многим». В этом режиме на вход сети последовательно подаются входные данные, и параллельно на выходе тоже получаем последовательность выходных значений. Обучение последовательностям «многие ко многим» можно использовать для машинного перевода, когда входная последовательность кодирует слова на одном языке, а выходная последовательность – на каком-то другом. В режиме «многие к одному» у нас есть последовательность в качестве входных данных, и мы должны предсказать единственный выход. Одним из вариантов использования такого режима является анализ тональности или классификация текста. В режиме «один ко многим» имеется только один входной образ, или входные данные имеются только для одного временного шага, а выходные данные содержат вектор из нескольких значений или для нескольких временных шагов. Такой режим может применяться, например, для генерации аудиозаписи. На вход подаем жанр музыки, который хотим получить, на выходе получаем аудиозапись. Также такой режим может применяться для создания заголовков или текстовых описаний изображений. В режиме «один к одному» имеется только одно входное и одно выходное значение, такой режим применяет очень редко.