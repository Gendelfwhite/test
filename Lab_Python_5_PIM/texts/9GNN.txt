Графовая нейронная сеть (Graph Neural Network, GNN) – тип нейронной сети, которая напрямую работает со структурой графа. Использование GNN позволяет работать с данными графов, без предварительной обработки. Такой подход позволяет сохранить топологические отношения между узлами графа. Самая фундаментальная часть графовых нейронных сетей – это граф. Граф – это структура данных, состоящая из двух компонентов: узлов (вершин) и ребер (см. рис. 76). То есть граф G можно определить как G = (V, E), где V – множество узлов, а E – ребра между ними. Если между узлами есть направленные зависимости, то ребра являются направленными. Если нет, то ребра ненаправленные. Граф может представлять собой такие вещи, как социальные сети или молекулы. В примере социальных сетей узлы – это пользователи, а ребра – связи между пользователями. Рис. 76. Основные элементы графов Графы часто представляется с помощи матрицы смежности. Если граф имеет n узлов, то матрица смежности имеет размерность (n × n) (см. рис. 77). Рис. 77. Матрица смежности 77 Иногда с узлами ассоциируются некоторые наборы данных (например, информация о профиле пользователя социальной сети). Если в узле есть f признаков, то матрица признаков узла X имеет размерность (n × f) (рис. 78). Рис. 78. Матрица смежности Графически структурированные данные присутствуют повсюду. Задачи, которые решают графовые нейронные сети, можно разделить на следующие категории: Классификация графов. Задача состоит в том, чтобы классифицировать весь граф по разным категориям. Это похоже на классификацию изображений. Приложения классификации графов многочисленны, например, от задачи определить, является ли белок ферментом или нет в биоинформатике, до классификации документов в НЛП или анализа социальных сетей. Классификация узлов. Задача состоит в том, чтобы определить характеристики узлов на основе их соседей. Обычно нейронные сети при решении этого типа задач обучаются полуконтролируемым образом, при этом маркируется только часть графа. Прогнозирование связей. При решении этой задачи алгоритм должен понимать взаимосвязь между объектами на графах, а также пытается предсказать, есть ли связь между двумя объектами. Например, в анализе социальных сетей важно делать выводы о социальных взаимодействиях или предлагать пользователям возможных друзей. Этот алгоритм также используется в задачах рекомендательных систем, и т. д. Кластеризация графов. Эта задача кластеризации данных, представленных в виде графов. Существуют две различные формы кластеризации данных графа. Кластеризация вершин стремится сгруппировать узлы графа в группы плотно связанных областей на основе либо 78 весов ребер, либо расстояний между ребрами. Вторая форма кластеризации графов рассматривает графы как объекты, подлежащие кластеризации, и группирует эти объекты на основе сходства. Вложение (embedding) графов. Это подход, который используется для преобразования узлов, ребер и их элементов в векторное пространство, т. е. в более низкое измерение, с максимальным сохранением таких свойств, как структура и информация о графе. Генерация графов. Генерация графов полезна для понимания процесса формирования графа, выявления аномальных частей графа, прогнозирования новых структур, моделирования новых структур графа, в некоторых случаях – построения полного графа в случае частично доступных данных графа. Генерация синтетических графов имеет реальные примеры использования, например, создание нового типа молекулы, которая излечивает определенное заболевание, учитывая класс молекул, таких, как токсичные/нетоксичные, и т. д. Графовые данные настолько сложны, что изначально создали множество проблем для существующих алгоритмов машинного обучения. Причина в том, что обычные инструменты машинного обучения и глубокого обучения специализируются на простых типах данных. Например, изображения с одинаковой структурой и размером мы можем рассматривать как сетки фиксированного размера. Текст и речь – это последовательности, поэтому мы можем представить их как линейный граф. Но есть и более сложные графы, без фиксированной формы, с переменным размером неупорядоченных узлов, где узлы могут иметь разное количество соседей (см. рис. 79). Также не помогает то, что существующие алгоритмы машинного обучения имеют основное предположение о том, что экземпляры данных независимы друг от друга. Это неверно для данных в виде графа, потому что каждый узел, может быть, связан с другими связями различных типов. Рис. 79. Представление различных видов данных в виде графов Рассмотрим основы глубокого обучения для графов на основе концепции векторного представления узлов. Это означает сопоставление узлов с d-мерным пространством (т. е. с малоразмерным пространством) таким образом, что похожие узлы в графе встраиваются близко 79 друг к другу в этом пространстве. Наша цель – отобразить узлы таким образом, чтобы сходство в векторном пространстве приближалось к сходству в сети. Функцией сходства может быть обычное евклидовое расстоянием. Определим u и v как два узла в графе. xu и xv – два вектора признаков, соответственно для узлов u и v. Теперь мы определим функцию кодировщика Enc(u) и Enc(v), которые преобразуют векторы признаков в векторы zu и zv (см. рис. 80) Необходимо отметить, что функция кодировщика должна иметь возможность выполнять: 1) локальные вычисление, т. е. выполнять вычисления для окрестности узла; 2) агрегировать информацию; 3) производить наложение и вычисление для нескольких слоев. Рис. 80. Вычислительный граф Информация о местоположении конкретного узла может быть получена с помощью вычислительного графа, так как он связан со своими соседями и соседями этих соседей. Мы знаем все возможные соединения и поэтому можем сформировать граф вычислений. Как только информация о местонахождении узла преобразована в вычислительный граф, мы можем производить агрегирование. В основном это делается с помощью обычных нейронных сетей. Эти нейронные сети представлены на рисунке 81 в виде серых прямоугольников. Операции агрегации должны быть инвариантны к порядку входных значений, например, это может быть сумма, среднее значение, максимум, потому что они являются инвариантными к перестановке функциями. Это свойство позволяет выполнять агрегирование. Рис. 81. Вычислительный граф 80 Рассмотрим алгоритм прямого распространения информации в многослойной графовой нейронной сети. Эта процедура вычислений определяет, как информация со входа будет поступать на выход нейронной сети. Каждый узел имеет вектор признаков. Например, XA – это вектор признаков узла A. Входными данными являются эти векторы признаков, в этом случае блок вычислений на основе двух векторов признаков (XA и Xc) объединит информацию о них и затем перейдет к следующему слою (см. рис. 82). Рис. 82. Многослойная графовая нейронная сеть Чтобы выполнить прямое распространение в этом вычислительном графе, нужно выполнить 3 шага: 1. Инициализация блоков активации: 2. На втором шаге для каждого уровня сети вычисление проводятся по формуле: Мы можем заметить, что это уравнение состоит из двух частей. Первая часть в основном усредняет по всем соседям узла v, здесь Wk – это матрица весовых коэффициентов k-го узла. Вторая часть вычисляет вложение предыдущего слоя узла v, умноженное на смещение Bk, которое представляет собой обучаемую весовую матрицу σ: нелинейная функция активации, которая выполняется для обоих частей. 3. Выходными значениями графовой нейронной сети являются значения h, вычисленные на последнем слое для каждого узла. Это вложение после K слоев агрегации окрестности узлов. Теперь, чтобы обучить модель, нам нужно определить функцию потерь для векторных представлений. Далее мы можем передать их в любую функцию потерь и запустить стохастический градиентный спуск для 81 обучения весовых параметров. Таким образом, в основе GNN заложен механизм распространения информации. Граф обрабатывается набором модулей, которые связаны между собой в соответствии со связями графа. В процессе обучения модули обновляют свои состояния и обмениваются информацией. Это продолжается до тех пор, пока модули не достигнут устойчивого равновесия. Выходные данные GNN вычисляются на основе состояния модуля на каждом узле. Построим модель простой графовой нейронной сети на основе PyTorch. Самая простая графовая нейронная сеть может состоять всего из трех разных последовательных операций: операции свертки графа, линейного слоя и нелинейной функции активации. Вместе они составляют один слой нейронной сети. Мы можем объединить эти слои в несколько слоев, чтобы сформировать многослойную графовую нейронную сеть. На рис. 83 представлена реализация простой графовой нейронной сети на языке Python на основе библиотеки PyTorch (https://github.com/fgafarov1977/pytorch_nn_tutorial/blob/main/gnn_simple.ipynb ). Рис. 83. Простая графовая нейронная сеть на языке Python Для работы с графовыми нейронными сетями на основе фреймворка PyTorch разработана библиотека PyTorch Geometric. Эта библиотека предоставляет удобные средства для представления и обработки графов в задачах машинного обучения. Библиотеку PyTorch Geometric необходимо предварительно установить, выполнив команду «pip install torch_geometric». На рис. 84 продемонстрировано создание и использования графовой нейронной сети на основе библиотеки Torch geometric для классификации статей в графе цитирования (данный пример расположен по адресу: https://github.com/fgafarov1977/pytorch_nn_tutorial/blob/main/gnn_cora.ipynb). Для решения этой задачи мы загружаем набор данных Cora и создаем простую двухслойную модель GCN, используя предопределенный слой GCNConv из библиотеки Torch geometric.